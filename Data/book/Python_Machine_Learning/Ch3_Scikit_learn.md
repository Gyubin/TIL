# Ch3. ML classifiers using Scikit-learn

## 0. ì„¤ì¹˜

ê°€ìƒí™˜ê²½ì„ ë¯¸ë¦¬ ë§Œë“ ë‹¤. numpy, scipyëŠ” prerequisiteì´ë¼ ë¨¼ì € ì„¤ì¹˜í•´ì¤˜ì•¼í•¨

- `pip3 install numpy`
- `pip3 install scipy`
- `pip3 install scikit-learn`

## 1. ê¸°ë³¸

- ëª¨ë“  ìƒí™©ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ì—†ë‹¤. ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ë³´ê³  ìµœì ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì•Œì•„ë‚´ì•¼í•œë‹¤.
- ì£¼ìš” ë‹¤ì„¯ ë‹¨ê³„
    + feature ì„ íƒ
    + ì„±ëŠ¥ metric ì„ íƒ 
    + classifier ì•Œê³ ë¦¬ì¦˜, optimization ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
    + ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    + ì•Œê³ ë¦¬ì¦˜ íŠœë‹
- ë°ì´í„°ëŠ” irisë¥¼ í™œìš©

    ```py
    from sklearn import datasets
    import numpy as np

    iris = datasets.load_iris()
    X = iris.data[:, [2, 3]]
    y = iris.target
    ```

- train, test ë°ì´í„° ë¶„ë¦¬: test ë°ì´í„° ì‚¬ì´ì¦ˆë¥¼ 0.3 ë¹„ìœ¨ë¡œ

    ```py
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
    ```

- feature scaling: ë°ì´í„°ë¥¼ í‘œì¤€í™”í•˜ëŠ” ê³¼ì •ì´ë‹¤.
    + `sc.fit(X_train)` : train ë°ì´í„°ì— ëŒ€í•´ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•œë‹¤.
    + `sc.transform(X_train)` : êµ¬í•´ì§„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ í™œìš©í•´ X_train ë°ì´í„°ë¥¼ í‘œì¤€í™”í•œë‹¤. X_test ë°ì´í„°ì— ëŒ€í•´ì„œë„ ë§ˆì°¬ê°€ì§€. X_trainì„ í™œìš©í•´ êµ¬í•œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ í™œìš©í•´ì„œ ë‘ ë°ì´í„° ëª¨ë‘ë¥¼ í‘œì¤€í™”í•˜ëŠ” ê²ƒì´ ì¤‘ìš”. ì¦‰ ê°™ì€ scaling parameterë¥¼ í™œìš©í•´ì•¼ ë¹„êµ ê°€ëŠ¥í•˜ë‹¤.

    ```py
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    sc.fit(X_train)
    X_train_std = sc.transform(X_train)
    X_test_std = sc.transform(X_test)
    ```

- ì‹œê°í™” í•¨ìˆ˜: ë¯¸ë¦¬ ì§œ ë‘”ë‹¤.

    ```py
    from matplotlib.colors import ListedColormap
    import matplotlib.pyplot as plt
    import warnings


    def versiontuple(v):
        return tuple(map(int, (v.split("."))))


    def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):

        # setup marker generator and color map
        markers = ('s', 'x', 'o', '^', 'v')
        colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
        cmap = ListedColormap(colors[:len(np.unique(y))])

        # plot the decision surface
        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                               np.arange(x2_min, x2_max, resolution))
        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
        Z = Z.reshape(xx1.shape)
        plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
        plt.xlim(xx1.min(), xx1.max())
        plt.ylim(xx2.min(), xx2.max())

        for idx, cl in enumerate(np.unique(y)):
            plt.scatter(x=X[y == cl, 0], 
                        y=X[y == cl, 1],
                        alpha=0.6, 
                        c=cmap(idx),
                        edgecolor='black',
                        marker=markers[idx], 
                        label=cl)

        # highlight test samples
        if test_idx:
            # plot all samples
            if not versiontuple(np.__version__) >= versiontuple('1.9.0'):
                X_test, y_test = X[list(test_idx), :], y[list(test_idx)]
                warnings.warn('Please update to NumPy 1.9.0 or newer')
            else:
                X_test, y_test = X[test_idx, :], y[test_idx]

            plt.scatter(X_test[:, 0],
                        X_test[:, 1],
                        c='',
                        alpha=1.0,
                        edgecolor='black',
                        linewidths=1,
                        marker='o',
                        s=55, label='test set')
    ```

## 2. Perceptron

### 2.1 í•™ìŠµ

```py
from sklearn.linear_model import Perceptron
ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)
ppn.fit(X_train_std, y_train)

y_pred = ppn.predict(X_test_std)
print("Misclassified samples: %d" % (y_test != y_pred).sum())
```

- `ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)` : ëª¨ë¸ ìƒì„±
    + epochs(ë°˜ë³µ)ëŠ” 40ë²ˆ, learning rateì€ 0.1
    + random_stateì— seed ê°’(ì •ìˆ˜)ì„ ì§€ì •í•´ì„œ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë„ ê°™ì€ ê°’ì´ ë‚˜ì˜¤ë„ë¡ í•¨
- `ppn.fit(X_train_std, y_train)` : train ë°ì´í„°ë¡œ í•™ìŠµ
- `y_pred = ppn.predict(X_test_std)`
    + í•™ìŠµëœ ppnì„ í™œìš©í•´ì„œ X_test_std ë°ì´í„°ì˜ ë‹µì„ ì˜ˆì¸¡í•œë‹¤.
    + ì‹¤ì œ ë‹µì¸ y_testì™€ ì˜ˆì¸¡ê°’ y_predì˜ ê°’ì„ ë¹„êµí•´ë³´ë©´ 45ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ 41ê°œë¡œ ì•½ 91.1 í¼ì„¼íŠ¸ì˜ ì •í™•ë„ë‹¤.

### 2.2 ì„±ëŠ¥ ì¸¡ì •

```py
from sklearn.metrics import accuracy_score
print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))
```

ì‰½ê²Œ ì–¼ë§ˆë‚˜ ë§ê²Œ ì˜ˆì¸¡í–ˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

### 2.3 ì‹œê°í™”

```py
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X=X_combined_std, y=y_combined, classifier=ppn, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
```

ì°¨íŠ¸ ê²°ê³¼ë¬¼ì„ ë³´ë©´ ì• ì´ˆì— irisê°€ ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„ë˜ì§€ ëª»í•˜ëŠ” ë°ì´í„°ì˜€ë‹¤. ê·¸ë˜ì„œ perceptronìœ¼ë¡œëŠ” ìˆ˜ë ´í•˜ì§€ ëª»í•œë‹¤.

## 3. Logistic regression

### 3.1 Sigmoid function

```py
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

z = np.arange(-7, 7, 0.1)
phi_z = sigmoid(z)

plt.plot(z, phi_z)
plt.axvline(0.0, color='k')
plt.ylim(-0.1, 1.1)
plt.xlabel('z')
plt.ylabel('$\phi (z)$')

# y axis ticks and gridline
plt.yticks([0.0, 0.5, 1.0])
ax = plt.gca()
ax.yaxis.grid(True)

plt.tight_layout()
# plt.savefig('./figures/sigmoid.png', dpi=300)
plt.show()
```

- ë² ë¥´ëˆ„ì´ ì‹œí–‰ì—ì„œ ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” ì‚¬ê±´ì˜ í™•ë¥ ì„ `p`ë¼ í–ˆì„ ë•Œ ì´ `p`ì™€ ê·¸ë ‡ì§€ ì•Šì„ í™•ë¥  `1-p`ì˜ ë¹„ìœ¨ì„ odds ratioë¼ê³  í•œë‹¤.
- odds ratioì— logë¥¼ ì”Œìš°ë©´ logit í•¨ìˆ˜ê°€ ëœë‹¤. `logit(p) = log p/(1-p)`
- logit í•¨ìˆ˜ì˜ ì—­ìˆ˜ê°€ logistic í•¨ìˆ˜ì´ê³  sigmoid í•¨ìˆ˜ë¼ê³ ë„ í•œë‹¤.
- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ê³„ë‹¨í•¨ìˆ˜ê°€ ì„ í˜•í™”ëœ ëª¨ìŠµì´ë‹¤. xì¶•ì˜ ê°’ì´ 0ì¼ ë•Œ y ê°’ì´ 0.5ê°€ ë˜ê³ , ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒí•˜ê²Œëœë‹¤.
- ì¦‰ xì¶• ê°’ì´ 0ë³´ë‹¤ í¬ë©´ A, ì‘ìœ¼ë©´ B ì„ íƒ. ì´ë ‡ê²Œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ì¶œë ¥ê°’ì„ í†µí•´ ì„ íƒí•˜ëŠ” ë¶€ë¶„ì„ Quantizerë¼ê³  í•œë‹¤.

### 3.2 êµ¬í˜„

```py
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(C=1000.0, random_state=0)
lr.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=lr, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
# plt.savefig('./figures/logistic_regression.png', dpi=300)
plt.show()
```

- `LogisticRegression` ë©”ì†Œë“œë¥¼ í™œìš©í•´ì„œ ëª¨ë¸ ìƒì„±
- ë§¤ê°œë³€ìˆ˜ C
    + ë¶„ì‚°ê³¼ biasì˜ ê· í˜•ì ì„ ì°¾ê¸° ìœ„í•´ ì •ê·œí™”ë¥¼ í•˜ëŠ”ë° ê°•ë„ì˜ ì •ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    + logistic regressionì˜ cost functionì—ì„œ ë§ˆì§€ë§‰ì— ì •ê·œí™”í•­ì„ ë”í•˜ê²Œ ë˜ëŠ”ë° `lambda/2 * ||w||^2` ì´ ë•Œ lambdaë¥¼ ì •ê·œí™” parameterë¼ê³  í•œë‹¤.
    + `C = 1 / lambda` : CëŠ” lambdaì™€ ì‹ê³¼ ê°™ì€ ê´€ê³„ë¥¼ ê°€ì§. Cë¥¼ ê°ì†Œì‹œí‚¤ë©´ ì •ê·œí™” ê°•ë„ë¥¼ ì¦ê°€í•˜ëŠ” ê²ƒì´ê³ , ì¦ê°€ì‹œí‚¤ë©´ ì •ê·œí™” ê°•ë„ë¥¼ ë‚®ì¶˜ë‹¤.
- `lr.predict_proba(X_test_std[0, :].reshape(1, -1))` : ì›í•˜ëŠ” row ë°ì´í„°ë¥¼ ì„ íƒí•´ì„œ ì–´ëŠ ì¢…ì— ì†í•˜ëŠ”ì§€ í™•ë¥ ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì½”ë“œ ê²°ê³¼ì—ì„œëŠ” 93.7í¼ì„¼íŠ¸ë¡œ 3ë²ˆì§¸ ê°’ì´ ì„ íƒëœë‹¤.

### 3.3 ì •ê·œí™”, ì˜¤ë²„í”¼íŒ…

```py
weights, params = [], []
for c in np.arange(-5., 5.):
    lr = LogisticRegression(C=10.**c, random_state=0)
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[1])
    params.append(10**c)

weights = np.array(weights)
plt.plot(params, weights[:, 0], label='petal length')
plt.plot(params, weights[:, 1], linestyle='--', label='petal width')
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.legend(loc='upper left')
plt.xscale('log')
# plt.savefig('./figures/regression_path.png', dpi=300)
plt.show()
```

- overfitting
    + train ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì€ ì¢‹ì§€ë§Œ ê²½í—˜í•˜ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•´ì„  ì„±ëŠ¥ì´ ë‚®ì€ ë¬¸ì œ. ì¦‰ train ë°ì´í„°ì— ë„ˆë¬´ ìµœì í™”ë˜ì–´ ì¼ë°˜í™”ë˜ì§€ ëª»í•¨
    + ë¶„ì‚°ì´ ì‹¬í•˜ë‹¤ ë¼ê³  í‘œí˜„.
    + ë„ˆë¬´ ë§ì€ featureë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ë°œìƒ
- underfitting
    + train ë°ì´í„° ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ê²½í—˜í•˜ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì„±ëŠ¥ ìì²´ê°€ ë–¨ì–´ì§€ëŠ” ë¬¸ì œ
    + biasê°€ ì‹¬í•˜ë‹¤ ë¼ê³  í‘œí˜„
    + featureë¥¼ ë„ˆë¬´ ì ê²Œ í•˜ë©´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
- ì •ê·œí™”: parameterê°€ ë„ˆë¬´ ì‘ê±°ë‚˜ í° ê°’ì„ ê°€ì§€ë©´ ì˜ëª» í•™ìŠµì´ ë  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ì›ë˜ì˜ ëª©í‘œí•¨ìˆ˜ì— ëª¨ë“  weightì˜ ì œê³±ì˜ í•©ì„ ë”í•˜ê±°ë‚˜(ridge regression), ê·¸ëƒ¥ ì ˆëŒ€ê°’ì„ ë”í•˜ê±°ë‚˜(lasso) í•´ì„œ ì •ê·œí™”í•œë‹¤.
- ìœ„ ë„í‘œëŠ” C ê°’ì— ë”°ë¼ coefficient(ê³„ìˆ˜) ê°’ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤. Cì˜ ê°’ì„ ëŠ˜ë¦¬ë©´ ì •ê·œí™” ê°•ë„ ì¦ê°€ë¥¼ ì˜ë¯¸í•œë‹¤.

### 4. Support Vector Machine

```py
from sklearn.svm import SVC

svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=svm, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
# plt.savefig('./figures/support_vector_machine_linear.png', dpi=300)
plt.show()
```

- SVM ì•Œê³ ë¦¬ì¦˜ì˜ ëª©í‘œëŠ” marginì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ
- hyperplane: pì°¨ì›ì—ì„œ ì„ í˜•í•¨ìˆ˜. ì¦‰ pì°¨ì›ì—ì„œ ê°’ë“¤ì„ ë¶„ë¦¬í•˜ëŠ” ê¸°ì¤€ì´ ë˜ëŠ” ê²ƒ. 2ì°¨ì›ì—ì„  ì§ì„ ì´ê³ , 3ì°¨ì›ì—ì„  í‰ë©´. í•´ë‹¹ í•¨ìˆ˜ë³´ë‹¤ ê°’ì´ í° ì˜ì—­ì„ positive hyperplane, ì‘ì€ ì˜ì—­ì„ negative hyperplaneì´ë¼ í•œë‹¤.
- Support vector: hyperplaneê³¼ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° ìƒ˜í”Œë“¤
- margin: positive hyperplaneì˜ support vectorì™€ negative hyperplaneì˜ support vector ê°„ì˜ ê±°ë¦¬
    + marginì´ í¬ë‹¤: ì¼ë°˜í™” ì˜¤ì°¨ê°€ ë‚®ë‹¤.
    + marginì´ ì‘ë‹¤: overfitting ê²½í–¥ì´ í¬ë‹¤.
- ìœ„ì—ì„œ ì–¸ê¸‰í•œ `C`ì˜ ê°’ì„ í™œìš©í•´ì„œ marginì˜ ë„ˆë¹„ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.

![Imgur](http://i.imgur.com/HYwq3rz.png)

- ë°ì´í„°ê°€ êµ‰ì¥íˆ í´ ë•ŒëŠ” ì»´í“¨í„° ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì„œ ì‘ì—…í•˜ê¸°ê°€ ì í•©íˆì§€ ì•Šë‹¤. SGDClassifier í´ë˜ìŠ¤ë¥¼ í™œìš©í•˜ë©´ ì´ ë¬¸ì œë¥¼ í•´ê²° ê°€ëŠ¥.
    + ë¼ì´ë¸ŒëŸ¬ë¦¬ : `from sklearn.linear_model import SGDClassifier`
    + perceptron : `ppn = SGDClassifier(loss='perceptron')`
    + logistic regression : `lr = SGDClassifier(loss='log')`
    + svm : `svm = SGDClassifier(loss='hinge')`

## 5. Non-linear with kernel SVM

### 5.1 kernelì— ëŒ€í•´ì„œ

[ë‹¤í¬ í”„ë¡œê·¸ë˜ë¨¸ ë¸”ë¡œê·¸](http://darkpgmr.tistory.com/147)ì— ê°€ë©´ kernelì— ëŒ€í•´ ë§¤ìš° ìì„¸í•˜ê²Œ ì„¤ëª…ë˜ì–´ìˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.

![kernel](https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Kernels.svg/1180px-Kernels.svg.png)

> kernel functionì˜ ì¢…ë¥˜. ì¶œì²˜: ìœ„í‚¤í”¼ë””ì•„

- kernel function: ìˆ˜í•™ì ìœ¼ë¡œ ì›ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ì´ë©´ì„œ ì ë¶„ê°’ì´ 1ì¸ non-negative í•¨ìˆ˜. ê°€ìš°ì‹œì•ˆ, Epanechnikov, uniform í•¨ìˆ˜ê°€ ëŒ€í‘œì 
- KDE(Kernel Density Estimation): ë°€ë„ ì¶”ì • ë°©ì‹ ì¤‘ non-parametric ë°©ì‹ì˜ í•˜ë‚˜ë‹¤. ê¸°ì¡´ non-parametric ë°©ì‹ ì¤‘ í•˜ë‚˜ì¸ íˆìŠ¤í† ê·¸ë¨ì´ ê²½ê³„ì—ì„œ ë¶ˆì—°ì†ì ì´ê¸° ë•Œë¬¸ì— kernel functionì„ ì´ìš©í•˜ì—¬ smoothening í•œ ê²ƒ.
- ì–´ë–»ê²Œ smoothening í•  ê²ƒì¸ê°€

    ![Imgur](http://i.imgur.com/6yLXt2e.png)

    + ìœ„ PDF(Probability Density Function)ì—ì„œ xëŠ” random variable, xiëŠ” ê´€ì¸¡ëœ ìƒ˜í”Œ ë°ì´í„°, KëŠ” ì»¤ë„ì´ë‹¤. hëŠ” ì»¤ë„ì´ ë¾°ì¡±í•œ í˜•íƒœ(ì‘ì€ ê°’)ì¸ì§€ ì™„ë§Œí•œ(í° ê°’) í˜•íƒœì¸ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    + ê´€ì¸¡ëœ ë°ì´í„° ê°ê°ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ëŠ” ì»¤ë„ í•¨ìˆ˜ë¥¼ ìƒì„±í•œë‹¤. `K(x-xi)`
    + ë§Œë“¤ì–´ì§„ ëª¨ë“  í•¨ìˆ˜ë¥¼ ë”í•´ì„œ ì „ì²´ ë°ì´í„° ê°œìˆ˜ë¡œ ë‚˜ëˆˆë‹¤.

![kernel-func](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Comparison_of_1D_histogram_and_KDE.png/1000px-Comparison_of_1D_histogram_and_KDE.png)

- h ê°’ì— ë”°ë¼ smoothing ì •ë„ê°€ ë‹¬ë¼ì§„ë‹¤. íšŒìƒ‰(true density: standard normal), ë¹¨ê°•, ê²€ì •, ë…¹ìƒ‰ ìˆœìœ¼ë¡œ h ê°’ì€ 0.05, 0.337, 2ë‹¤. ë†’ì•„ì§ˆìˆ˜ë¡ ì™„ë§Œí•´ì§.

![kde-h](https://upload.wikimedia.org/wikipedia/en/thumb/e/e5/Comparison_of_1D_bandwidth_selectors.png/440px-Comparison_of_1D_bandwidth_selectors.png)

- KDEë¥¼ í™œìš©í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ì–´ë–¤ ì»¤ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í• ì§€ì™€ hê°’ì„ ì–´ë–»ê²Œ ì„¤ì •í• ì§€ë¼ê³  í•œë‹¤.
    + ìµœì ì€ Epanechnikov ì»¤ë„ í•¨ìˆ˜
    + ê°€ìš°ì‹œì•ˆë„ ë§ì´ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë•Œ h ê°’ì€ `h = ((4 * ğ›”^5) / 3n)^(1/5) = (1.06 * ğ›”n)^(-1/5)`

    ![h value](https://wikimedia.org/api/rest_v1/media/math/render/svg/9ec402653306a6af7383bc50062be20d557508b2)

### 5.2 XOR ë°ì´í„° ë§Œë“¤ê¸°

```py
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(0)
X_xor = np.random.randn(200, 2)
y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)
y_xor = np.where(y_xor, 1, -1)

plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],
            c='b', marker='x', label='1')
plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1],
            c='r', marker='s', label='-1')

plt.xlim([-3, 3])
plt.ylim([-3, 3])
plt.legend(loc='best')
plt.tight_layout()
# plt.savefig('./figures/xor.png', dpi=300)
plt.show()
```

- `X_xor = np.random.randn(200, 2)` : 200 by 2 ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë§Œë“œëŠ”ë° ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ë‚œìˆ˜ë¡œ í•œë‹¤.
- `X_xor[:, 0] > 0`, `X_xor[:, 1] > 0` : ê°ê° X_xorì˜ 0ë²ˆ ì»¬ëŸ¼, 1ë²ˆ ì»¬ëŸ¼ì´ 0ë³´ë‹¤ ì´ìƒì´ë©´ True, ì•„ë‹ˆë©´ Falseë¡œ ê°’ì„ ì„¤ì •í•´ì„œ ìƒˆë¡œìš´ np.arrayë¥¼ ë¦¬í„´í•œë‹¤.
- `y_xor = np.logical_xor(data1, data2)` : xor ì—°ì‚°ì„ í–‰í•œ ê²°ê³¼ë¥¼ ë¦¬í„´í•œë‹¤. data1, 2ëŠ” ê°™ì€ í¬ê¸°ì—¬ì•¼í•¨. ì¦‰ y_xorì˜ ê°’ì€ X_xor ë°ì´í„°ì˜ ê° í–‰(x,y ì¢Œí‘œ)ì´ Trueì¸ì§€ Falseì¸ì§€ ê°’ì„ ê°€ì§€ê³  ìˆë‹¤.
- `plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1])` : y_xorì˜ ê°’ì´ 1ì¸ ì§€ì , ì¦‰ xorì—°ì‚°ì´ Trueì¸ ì¢Œí‘œë¥¼ ê³¨ë¼ì„œ ì ì„ ì°ëŠ”ë‹¤.

### 5.3 ê³ ì°¨ì› ê³µê°„ì—ì„œ hyperplane ì°¾ê¸°

- train ë°ì´í„°ë¡œ í•™ìŠµí•˜ê¸°
    + ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ì—†ëŠ” ë°ì´í„°ëŠ” ê¸°ì¡´ì— ê°€ì§€ê³  ìˆë˜ featureë“¤ì„ ì¡°í•©í•´ì„œ ìƒˆë¡œìš´ ë¹„ì„ í˜• featureë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.
    + `mapping function Ï•(x1, x2) = (z1, z2, z3) = (x1, x2, x1^2 + x2^2)` : ì´ë ‡ê²Œ ê³ ì°¨ì›ìœ¼ë¡œ ë°”ê¾¸ë©´ ì‹ ê¸°í•˜ê²Œë„ 3ì°¨ì›ì—ì„œ hyperplaneì´ ì •í™•í•˜ê²Œ ì¤‘ê°„ ì¢Œí‘œë“¤ë§Œ 'ë†’ì´'ê°’ì„ í™œìš©í•´ì„œ ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.
    + ê³ ì°¨ì›ì—ì„œì˜ ì„ í˜• SVM ëª¨ë¸ì„ í•™ìŠµì‹œì¼œë‘”ë‹¤.
- test ë°ì´í„° ê²€ì¦í•˜ê¸°
    + test ë°ì´í„°ë¥¼ train ë°ì´í„°ì²˜ëŸ¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê³ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•œ í›„ ê¸°ì¡´ ëª¨ë¸ì„ í™œìš©í•´ ë¶„ë¥˜í•œë‹¤.
    + í•´ë‹¹ ë°ì´í„°ë¥¼ ë‹¤ì‹œ 2ì°¨ì›ìœ¼ë¡œ ë˜ëŒë ¤ì„œ í™œìš©

### 5.4 kernel trick

```py
# Using xor dataset
svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)

plot_decision_regions(X_xor, y_xor, classifier=svm)
plt.legend(loc="upper left")
plt.show()
```

- ìœ„ 5.3ì˜ ê³ ì°¨ì› íˆ¬ì˜ ë°©ì‹ì€ ê³„ì‚°ëŸ‰ì´ ë§¤ìš° í¬ê²Œ ëŠ˜ì–´ë‚œë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
- ê·¸ë˜ì„œ ê¸°ì¡´ `(xi)T * xj`ê°€ ê³ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜ë¼ `Ï•((xi)T) * Ï•(xj)`ë¡œ ëŒ€ì²´ë˜ëŠ” ê³¼ì • ëŒ€ì‹  kernel function Kë¥¼ í™œìš©í•œë‹¤.
- ì´ë¥¼ kernel trickì´ë¼ í•˜ê³  ê°€ì¥ ìì£¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ RBF kernel(Radial Basis Function kernel)ê³¼ Gaussian kernelì´ë‹¤.
    + ![rbf-kernel](https://wikimedia.org/api/rest_v1/media/math/render/svg/c16fd6c515412f96a57506103896178d0e8af77d)
    + ìœ„ ê³µì‹ì€ RBF kernelì´ê³  ë¶„ëª¨ë¥¼ gammaë¡œ ì¹˜í™˜í•´ì„œ ê°„ì†Œí™”í•˜ê¸°ë„ í•œë‹¤. ì´ gamma ê°’ì„ ì¡°ì •í•´ê°€ë©´ì„œ ìµœì í™”í•´ì•¼í•œë‹¤.
- RBFë¥¼ í™œìš©í•˜ëŠ” ìœ„ ì½”ë“œëŠ” XORì„ ì •í™•í•˜ê²Œ ë¶„ë¦¬í•´ë‚¸ë‹¤.

### 5.5 irisì— kernel trick ì ìš©

```py
svm = SVC(kernel='rbf', random_state=0, gamma=0.2, C=1.0)
# svm = SVC(kernel='rbf', random_state=0, gamma=100.0, C=1.0)
svm.fit(X_train_std, y_train)
plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [stanardized]')
plt.legend(loc='upper left')
plt.show()
```

- gamma ê°’ì´ í¬ë©´ ê²½ê³„ê°€ ë¶€ë“œëŸ¬ì›Œì§€ê³ , ì‘ìœ¼ë©´ ê²½ê³„ê°€ íƒ€ì´íŠ¸í•´ì§„ë‹¤.
- ë„ˆë¬´ ê°’ì„ ì‘ê²Œí•´ì„œ ê²½ê³„ë¥¼ íƒ€ì´íŠ¸í•˜ê²Œ í•˜ë©´ overfitting ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

## 6. Decision tree

### 6.1 ê¸°ë³¸

![decision-tree](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/CART_tree_titanic_survivors_KOR.png/700px-CART_tree_titanic_survivors_KOR.png)

- ìœ„ ì´ë¯¸ì§€ì²˜ëŸ¼ ì¼ë ¨ì˜ "ì§ˆë¬¸"ë“¤ì„ ê±°ì³ì„œ ë°ì´í„°ë¥¼ ìª¼ê°œê°€ëŠ” ê³¼ì •ì´ë‹¤. ë‹¤ë¥¸ ë¶„ì„ë“¤ì— ë¹„í•´ ì§ê´€ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ë‹¤. ë‰´ëŸ´ë„·ì€ hidden layerê°€ ìˆëŠ” ë¶„ì„ê°€ê°€ ì•Œê¸° ì–´ë ¤ìš´ ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ì´ì§€ë§Œ decision treeëŠ” í™”ì´íŠ¸ë°•ìŠ¤ ëª¨ë¸ì´ë‹¤.
- ê³„ì‚° ë¹„ìš©ì´ ë‚®ì•„ì„œ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì—ì„œë„ ë¹ ë¥´ê²Œ ì—°ì‚° ê°€ëŠ¥
- ê°€ì¥ í° IG(Information Gain) ê°’ì„ ë„ì¶œí•˜ëŠ” featureë¥¼ ì°¾ê³  ì´ë¥¼ í†µí•´ ë…¸ë“œë¥¼ ë¶„ë¥˜í•œë‹¤.
- íŠ¸ë¦¬ê°€ ë„ˆë¬´ ê¹Šì–´ì§€ë©´ overfitting ë¬¸ì œê°€ ë°œìƒí•˜ë¯€ë¡œ ìµœëŒ€ ê¹Šì´ ì œí•œì„ ë‘”ë‹¤.
- ì¢…ë¥˜
    + regression tree : ì¶œë ¥ì´ ì—°ì†í˜•(ìˆ«ì)
    + classification tree : ì¶œë ¥ì´ ë²”ì£¼í˜•

### 6.2 êµ¬í˜„

```py
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(criterion="entropy", max_depth=3, random_state=0)
tree.fit(X_train, y_train)

X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree, test_idx=range(105, 150))
plt.xlabel('petal length [cm]')
plt.ylabel('petal width [cm]')
plt.legend(loc='upper left')
plt.show()
```

### 6.3 ë„í‘œ export

```py
from sklearn.tree import export_graphviz
export_graphviz(tree, out_file="tree.dot", feature_names=['petal length', 'petal width'])
```

- [graphviz](http://graphviz.org/)ì—ì„œ ë‹¤ìš´ë°›ê³  ì„¤ì¹˜
- `dot -Tpng tree.dot -o tree.png` : ì™¼ìª½ ëª…ë ¹ìœ¼ë¡œ ìœ„ ì½”ë“œì—ì„œ ë§Œë“  dot íŒŒì¼ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜

## 7. Random forests

### 7.1 ê¸°ë³¸

- decision treeì˜ ì•™ìƒë¸” ë²„ì „ì´ë‹¤.(treeê°€ ì—¬ëŸ¬ê°œ ëª¨ì—¬ì„œ forests)
- ì•™ìƒë¸”(Ensemble): ì•™ìƒë¸”ì€ ì•½í•œ í•™ìŠµê¸°ë¥¼ ì—¬ëŸ¿ ê²°í•©í•´ì„œ ê°•í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê°œë…. ê·¸ë˜ì„œ ì¼ë°˜í™”í•˜ê¸° ì‰½ê³  overfittingì´ ì˜ ì•ˆë  ìˆ˜ ìˆì–´ì„œ ì¢‹ë‹¤.
- ë°©ì‹
    + nê°œì˜ ìƒ˜í”Œ 

### 7.2 êµ¬í˜„

```py
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)
forest.fit(X_train, y_train)

plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105, 150))
plt.xlabel('petal length')
plt.ylabel('petal width')
plt.legend(loc='upper left')
plt.show()
```

- `criterion="entropy"` : ë¶ˆìˆœë„ ì¸¡ì • ê¸°ì¤€ìœ¼ë¡œ ì—”íŠ¸ë¡œí”¼ ì‚¬ìš©
- `n_estimators` : ê²°í•©í•  decision treeì˜ ê°œìˆ˜
- `n_jobs` : ì½”ì–´ ëª‡ ê°œë¥¼ ì“¸ê±´ì§€

## 8. KNN(K-nearest neighbor)

### 8.1 ê¸°ë³¸

- lazy leraner: KNNì€ lazy learner ë¥˜ì˜ ë°©ì‹ì´ë‹¤. train ë°ì´í„°ë¥¼ í™œìš©í•´ì„œ í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ë°ì´í„°ë¥¼ ê¸°ì–µí•˜ê¸° ë•Œë¬¸
- parametric model
    + ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸, ì¦‰ í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ train ë°ì´í„°ì…‹ì—ì„œ parameterë¥¼ ì¶”ì •í•œë‹¤. í•™ìŠµì´ ëë‚œ í›„ train ë°ì´í„°ì…‹ì€ ë”ì´ìƒ í•„ìš”ì—†ë‹¤.
    + ì‚¬ë¡€: Perceptron, Logistic regression, linear SVM
- nonparametric model
    + parameterê°€ ë”± ì •í•´ì§„ ê²ƒì´ ì•„ë‹ˆë¼ train ë°ì´í„°ì…‹ê³¼ í•¨ê»˜ ê°œìˆ˜ê°€ ì¦ê°€í•œë‹¤. parametric modelì— ë¹„í•´ ê³„ì‚° ë¹„ìš©ì´ ë†’ë‹¤.
    + ì‚¬ë¡€: decision tree classifier, random forest, kernel SVM, KNN
- KNNì€ nonparametric modelì˜ í•˜ë‚˜ì´ê³ , instance-based learningì´ë‹¤. instance-based learningì€ í›ˆë ¨ ë°ì´í„°ì…‹ì„ "ê¸°ì–µ"í•˜ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤.
- lazy learningì€ instance-based learning ì¤‘ì—ì„œë„ í•™ìŠµ ê³¼ì •ì—ì„œ costê°€ ì—†ëŠ” íŠ¹ë³„í•œ ì¼€ì´ìŠ¤ë‹¤. "í•™ìŠµ"í•  í•„ìš”ê°€ ì—†ì–´ì„œ í•™ìŠµì— ëŒ€í•œ costëŠ” ì—†ëŠ”ê±°ì§€ë§Œ ë§‰ìƒ ë¶„ë¥˜í•  ë•ŒëŠ” ë‹¤ë¥¸ ë°©ì‹ì— ë¹„í•´ costê°€ ë†’ë‹¤. ìƒ˜í”Œ ìˆ«ìê°€ ì»¤ì§€ë©´ ë”ë”ìš± ë¶„ë¥˜ costê°€ ì»¤ì§„ë‹¤.

### 8.2 ë¶„ë¥˜ ë°©ì‹

- kì— í•´ë‹¹í•˜ëŠ” ìˆ«ìì™€ ê±°ë¦¬ ë©”íŠ¸ë¦­ ì„ íƒ. kê°’ì— ë”°ë¼ overfitting, underfittingì´ ë‹¬ë¼ì§„ë‹¤.
- ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” ìƒ˜í”Œì— ëŒ€í•œ kê°œì˜ ê·¼ì ‘í•œ ì´ì›ƒ ì°¾ê¸°
- ë‹¤ìˆ˜ê²° íˆ¬í‘œ ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜ ë ˆì´ë¸” í• ë‹¹í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ kê°€ 5ë¼ë©´ ê·¼ì ‘ ì´ì›ƒì„ 5ê°œ ë½‘ì•„ì„œ ê·¸ ì¤‘ ê°€ì¥ ë§ì€ í˜•íƒœì˜ ë ˆì´ë¸”ì„ í• ë‹¹í•˜ëŠ” ê²ƒ.

### 8.3 êµ¬í˜„

```py
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5, p=2, metric="minkowski")
knn.fit(X_train_std, y_train)
plot_decision_regions(X_combined_std, y_combined, classifier=knn, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.show()
```

- `knn = KNeighborsClassifier(n_neighbors=5, p=2, metric="minkowski")`
    + `n_neighbors` : ëª‡ ê°œì˜ ì´ì›ƒì„ ì„ íƒí• ê±´ì§€. kì— í•´ë‹¹
    + `p` : 1ì´ë©´ ë§¨í•´íŠ¼ ê±°ë¦¬, 2ë©´ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬
    + `metric` : ê±°ë¦¬ ë©”íŠ¸ë¦­ ì„ íƒ
